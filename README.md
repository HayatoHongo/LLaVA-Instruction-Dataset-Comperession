# LLaVA-Instruction-Dataset-Compression

## Aim

This repository provides **compressed image datasets** used by
`llava_v1_5_mix665k.json`.

Original instruction dataset:
[https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)

The goal is to download, organize, and compress all related image datasets
into a single high-compression archive for efficient storage and reuse.

---

## Environment

* **Cloud provider**: Lambda Cloud
* **Instance**: A10 GPU instance, 0.75$/hour
* **Note**: GPU was *not* used

  * Large disk capacity
  * 30 vCPUs
  * High network throughput
    made this instance ideal for large-scale dataset downloading and compression.
  * Attatch filesystem

---

## Dataset Downloads

### COCO (train2017)

Initial downloads were very fast thanks to network boosting.

```bash
wget http://images.cocodataset.org/zips/train2017.zip
ls -lh train2017.zip   # ~19GB
unzip train2017.zip
mkdir -p coco
mv train2017 coco/
```

After this step, the download speed dropped significantly (likely due to
boosting quota exhaustion), and subsequent downloads took much longer
(around 1 hour each).

---

### GQA

```bash
wget https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip
ls -lh images.zip      # ~21GB
unzip images.zip
mkdir -p gqa
mv images gqa/
mv images.zip gqa_images.zip
```

---

### TextVQA

Download speed recovered.

```bash
wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip
ls -lh train_val_images.zip
unzip train_val_images.zip
mkdir -p textvqa
mv train_images textvqa/
```

---

### Visual Genome (VG)

#### VG_100K

```bash
wget https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip
ls -lh images.zip      # ~9.1GB
unzip images.zip
mkdir -p vg
mv VG_100K vg/
mv images.zip vg_images.zip
```

#### VG_100K_2

```bash
wget https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip
ls -lh images2.zip     # ~5.1GB
unzip images2.zip
mv VG_100K_2 vg/
```

---

### OCR-VQA (LLaVA v1.5)

```bash
wget https://huggingface.co/datasets/weizhiwang/llava_v15_instruction_images/resolve/main/ocr_vqa_images_llava_v15.zip
ls -lh ocr_vqa_images_llava_v15.zip   # ~8.4GB
unzip ocr_vqa_images_llava_v15.zip
mkdir -p ocr_vqa
mv images ocr_vqa/
```

---

## Final Directory Structure

After all downloads and extraction:

```
coco/
gqa/
textvqa/
vg/
ocr_vqa/
```

---

## Compression

All image datasets are compressed into a single archive using **zstd** with
maximum compression and multithreading:

```bash
tar -I 'zstd -19 -T0' \
  -cf images.tar.zst \
  coco gqa ocr_vqa textvqa vg
```

* `-19` : maximum compression level
* `-T0` : use all available CPU cores

---

## Upload to HuggingFace
```bash
pip install -U huggingface_hub
```

```bash
export HF_TOKEN="hf...."
echo $HF_TOKEN
```

HFAPI and so on does not have meaning!

---

# âœ… Hugging Face Dataset ã« 70GB ç”»åƒã‚’ 5GB shard ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å®Œå…¨æ‰‹é †ï¼ˆç¢ºå®šç‰ˆï¼‰

---

## 0ï¸âƒ£ å‰æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

```bash
~/LLaVA-Instruction-Dataset-Comperession/
â””â”€â”€ images.tar.zst          # å…ƒã® 70GB ãƒ•ã‚¡ã‚¤ãƒ«
```

---

## 1ï¸âƒ£ 70GB ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ 5GB shard ã«åˆ†å‰²

```bash
split -b 5G images.tar.zst images.tar.zst.part_
```

ç”Ÿæˆä¾‹ï¼š

```
images.tar.zst.part_aa
images.tar.zst.part_ab
...
images.tar.zst.part_an
```

---

## 2ï¸âƒ£ å¿…è¦ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
sudo apt update
sudo apt install -y git-lfs
```

---

## 3ï¸âƒ£ git-lfs ã‚’åˆæœŸåŒ–ï¼ˆæœ€åˆã«1å›ï¼‰

```bash
git lfs install
```

ç¢ºèªï¼š

```bash
git lfs version
```

---

## 4ï¸âƒ£ Hugging Face Dataset ãƒªãƒã‚¸ãƒˆãƒªã‚’ clone

```bash
git clone https://huggingface.co/datasets/HayatoHongoEveryonesAI/LLaVA-Instruction-665k-Images-Dataset
```

---

## 5ï¸âƒ£ Dataset ãƒªãƒã‚¸ãƒˆãƒªã«ç§»å‹•

```bash
cd LLaVA-Instruction-665k-Images-Dataset
```

---

## 6ï¸âƒ£ LFS ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°è¨­å®šï¼ˆæœ€é‡è¦ï¼‰

```bash
git lfs track "*.part_*"
git lfs track "*.zst"
```

---

## 7ï¸âƒ£ .gitattributes ã‚’ commit

```bash
git add .gitattributes
git commit -m "Track large dataset shards with git-lfs"
```

â€» åˆå›ã®ã¿ git identity ãŒå¿…è¦ï¼š

```bash
git config --global user.email "hayato.hongo@everyonesai.org"
git config --global user.name "HayatoHongoEveryonesAI"
```

---

## 8ï¸âƒ£ shard ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ

```bash
mkdir -p data/images
```

---

## 9ï¸âƒ£ shard ã‚’ repo å†…ã«ã‚³ãƒ”ãƒ¼ï¼ˆå®‰å…¨ãƒ»é«˜é€Ÿï¼‰

```bash
rsync -av --progress \
  ../images.tar.zst.part_* \
  data/images/
```

---

## ğŸ”Ÿ shard ã‚’ add â†’ commit

```bash
git add data/images/images.tar.zst.part_*
git commit -m "Add LLaVA image shards (5GB parts)"
```

---

## 1ï¸âƒ£1ï¸âƒ£ 5GB è¶… LFS ã‚’æ˜ç¤ºçš„ã«è¨±å¯ï¼ˆHF å›ºæœ‰ãƒ»å¿…é ˆï¼‰

```bash
hf lfs-enable-largefiles .
```

â€» **ã“ã‚Œã‚’å¿˜ã‚Œã‚‹ã¨å¿…ãš push å¤±æ•—**

---
---

## 1ï¸âƒ£2ï¸âƒ£  pushï¼ˆåˆ‡ã‚Œã¦ã‚‚OKã€å®Œèµ°ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™ï¼‰

```bash
git push
```

* `broken pipe` ãŒå‡ºã¦ã‚‚ **æ­£å¸¸**
* å†åº¦ï¼š

```bash
git push
```

---

## âœ… æˆåŠŸãƒ­ã‚°ï¼ˆã“ã‚ŒãŒå‡ºãŸã‚‰çµ‚äº†ï¼‰

```text
Uploading LFS objects: 100% (14/14), 72 GB | XXX MB/s, done.
To https://huggingface.co/datasets/HayatoHongoEveryonesAI/LLaVA-Instruction-665k-Images-Dataset
   xxxx..yyyy  main -> main
```

---

## ğŸ” æœ€çµ‚ç¢ºèªï¼ˆä»»æ„ï¼‰

```bash
git lfs ls-files
```

---

# ğŸ§  é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆå†ç™ºé˜²æ­¢ï¼‰

* **HFAPI ã¯ä½¿ã‚ãªã„**
* **5GB shard ãŒå®‰å®šä¸Šé™**
* `hf lfs-enable-largefiles .` ã¯ **å¿…é ˆ**
* `git push` ã¯ **å†å®Ÿè¡Œå‰æ**
* Web UI ã¯ä¿¡ç”¨ã—ãªã„

---

# Option: Copy to filesystem(if you attached) 

Please take it into account you will be charged based on your disk size in the filesystem. (maybe 20USD per month for 70GB storage)
```bash
rsync -ah --progress \
  /home/ubuntu/LLaVA-Instruction-Dataset-Comperession/images.tar.zst \
  /home/ubuntu/virginia-filesystem/
```


## Result

* A single highly compressed archive: `images.tar.zst`
* Suitable for:

  * Dataset sharing
  * Fast rehydration on new machines
  * Reduced storage and transfer costs
